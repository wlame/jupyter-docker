{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Audio Analysis\n",
    "\n",
    "Demonstrates audio processing and feature extraction with librosa and torchaudio.\n",
    "\n",
    "**Libraries:**\n",
    "- [librosa](https://librosa.org/) — Audio analysis: MFCCs, mel spectrogram, chroma, beat tracking, HPSS\n",
    "- [torchaudio](https://pytorch.org/audio/) — PyTorch-native audio transforms and resampling\n",
    "- [soundfile](https://python-soundfile.readthedocs.io/) — WAV/FLAC/OGG I/O\n",
    "\n",
    "Uses librosa's built-in Tchaikovsky Nutcracker sample — no external files needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Audio\n",
    "\n",
    "`librosa.ex()` returns paths to bundled audio samples (no download required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(librosa.ex('nutcracker'), duration=30.0)\n",
    "\n",
    "print(f\"Audio shape : {y.shape}\")\n",
    "print(f\"Sample rate : {sr} Hz\")\n",
    "print(f\"Duration    : {len(y)/sr:.2f} s\")\n",
    "print(f\"Value range : [{y.min():.4f}, {y.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waveform-header",
   "metadata": {},
   "source": [
    "## Waveform and Energy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waveform",
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr = librosa.feature.zero_crossing_rate(y)\n",
    "rms = librosa.feature.rms(y=y)\n",
    "\n",
    "times = np.linspace(0, len(y) / sr, len(y))\n",
    "zcr_times = librosa.times_like(zcr, sr=sr)\n",
    "rms_times = librosa.times_like(rms, sr=sr)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "axes[0].plot(times, y, linewidth=0.4, color='steelblue')\n",
    "axes[0].set_title(\"Waveform\")\n",
    "axes[0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "axes[1].semilogy(zcr_times, zcr[0], color='darkorange', linewidth=0.8)\n",
    "axes[1].set_title(f\"Zero-Crossing Rate (mean={zcr.mean():.4f})\")\n",
    "axes[1].set_ylabel(\"ZCR\")\n",
    "\n",
    "axes[2].plot(rms_times, rms[0], color='seagreen', linewidth=0.8)\n",
    "axes[2].set_title(f\"RMS Energy (mean={rms.mean():.4f})\")\n",
    "axes[2].set_xlabel(\"Time (s)\")\n",
    "axes[2].set_ylabel(\"Energy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpss-header",
   "metadata": {},
   "source": [
    "## Harmonic-Percussive Source Separation (HPSS)\n",
    "\n",
    "HPSS decomposes audio into tonal (harmonic) and rhythmic (percussive) layers.\n",
    "Using them separately improves downstream feature quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hpss",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "\n",
    "print(f\"Harmonic energy  : {np.sum(y_harmonic**2):.2f}\")\n",
    "print(f\"Percussive energy: {np.sum(y_percussive**2):.2f}\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 7))\n",
    "for ax, sig, title, color in zip(\n",
    "    axes,\n",
    "    [y, y_harmonic, y_percussive],\n",
    "    ['Original', 'Harmonic Component', 'Percussive Component'],\n",
    "    ['steelblue', 'royalblue', 'tomato'],\n",
    "):\n",
    "    ax.plot(np.linspace(0, len(sig)/sr, len(sig)), sig, linewidth=0.3, color=color)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beat-header",
   "metadata": {},
   "source": [
    "## Beat Tracking and Tempo Estimation\n",
    "\n",
    "librosa uses the percussive component for more stable beat detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beats",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo, beat_frames = librosa.beat.beat_track(y=y_percussive, sr=sr)\n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "tempo_val = float(tempo) if np.ndim(tempo) == 0 else float(tempo[0])\n",
    "\n",
    "print(f\"Estimated tempo : {tempo_val:.1f} BPM\")\n",
    "print(f\"Number of beats : {len(beat_times)}\")\n",
    "print(f\"Average beat interval: {np.mean(np.diff(beat_times)):.3f} s\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(times, y, linewidth=0.4, color='steelblue', alpha=0.7, label='Waveform')\n",
    "for bt in beat_times:\n",
    "    ax.axvline(x=bt, color='red', alpha=0.5, linewidth=0.7)\n",
    "ax.axvline(x=beat_times[0], color='red', alpha=0.9, linewidth=0.7, label='Beat')\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "ax.set_title(f\"Beat Tracking — {tempo_val:.1f} BPM\")\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectral-header",
   "metadata": {},
   "source": [
    "## Spectral Features: Mel Spectrogram, MFCC, Chroma\n",
    "\n",
    "| Feature | Description | Use Case |\n",
    "|---------|-------------|----------|\n",
    "| **Mel Spectrogram** | Frequency energy on perceptual scale | General audio representation |\n",
    "| **MFCC** | Compact spectral shape descriptors | Speech recognition, genre classification |\n",
    "| **Chroma** | Pitch class energy (C, C#, D, ...) | Harmony, chord recognition |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectral-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length = 512\n",
    "n_mfcc = 13\n",
    "\n",
    "mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, hop_length=hop_length)\n",
    "mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "mfcc = librosa.feature.mfcc(y=y_harmonic, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "mfcc_delta = librosa.feature.delta(mfcc)\n",
    "chroma = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr, hop_length=hop_length)\n",
    "\n",
    "centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length)\n",
    "bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=hop_length)\n",
    "rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=hop_length, roll_percent=0.85)\n",
    "\n",
    "print(f\"Mel spectrogram : {mel_spec.shape}\")\n",
    "print(f\"MFCC            : {mfcc.shape}\")\n",
    "print(f\"Chroma          : {chroma.shape}\")\n",
    "print(f\"\\nSpectral centroid mean : {centroid.mean():.1f} Hz\")\n",
    "print(f\"Spectral bandwidth mean: {bandwidth.mean():.1f} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectral-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, figsize=(13, 14))\n",
    "\n",
    "img0 = librosa.display.specshow(mel_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', ax=axes[0])\n",
    "axes[0].set_title(\"Mel Spectrogram (dB)\")\n",
    "fig.colorbar(img0, ax=axes[0], format=\"%+2.0f dB\")\n",
    "\n",
    "img1 = librosa.display.specshow(mfcc, sr=sr, hop_length=hop_length, x_axis='time', ax=axes[1])\n",
    "axes[1].set_title(f\"MFCCs (first {n_mfcc} coefficients)\")\n",
    "axes[1].set_ylabel(\"MFCC coefficient\")\n",
    "fig.colorbar(img1, ax=axes[1])\n",
    "\n",
    "img2 = librosa.display.specshow(chroma, sr=sr, hop_length=hop_length, x_axis='time', y_axis='chroma', ax=axes[2])\n",
    "axes[2].set_title(\"Chroma Features (CQT) — Pitch Class Energy\")\n",
    "fig.colorbar(img2, ax=axes[2])\n",
    "\n",
    "feat_times = librosa.times_like(centroid, sr=sr, hop_length=hop_length)\n",
    "axes[3].plot(feat_times, centroid[0], label='Centroid', linewidth=0.8, color='steelblue')\n",
    "axes[3].plot(feat_times, rolloff[0], label='Rolloff (85%)', linewidth=0.8, color='darkorange')\n",
    "axes[3].fill_between(feat_times, centroid[0]-bandwidth[0], centroid[0]+bandwidth[0],\n",
    "                     alpha=0.25, color='steelblue', label='±Bandwidth')\n",
    "axes[3].set_xlabel(\"Time (s)\")\n",
    "axes[3].set_ylabel(\"Frequency (Hz)\")\n",
    "axes[3].set_title(\"Spectral Centroid, Bandwidth, and Rolloff\")\n",
    "axes[3].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beat-sync-header",
   "metadata": {},
   "source": [
    "## Beat-Synchronised Feature Matrix\n",
    "\n",
    "Aggregating frame-level features at beat boundaries reduces sequence length and\n",
    "aligns features with musical structure — useful for song-level classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beat-sync",
   "metadata": {},
   "outputs": [],
   "source": [
    "beat_mfcc = librosa.util.sync(mfcc, beat_frames, aggregate=np.median)\n",
    "beat_chroma = librosa.util.sync(chroma, beat_frames, aggregate=np.median)\n",
    "beat_features = np.vstack([beat_chroma, beat_mfcc])\n",
    "\n",
    "print(f\"Beat-sync chroma : {beat_chroma.shape}\")\n",
    "print(f\"Beat-sync MFCC   : {beat_mfcc.shape}\")\n",
    "print(f\"Combined (25 x beats): {beat_features.shape}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "img = ax.imshow(beat_features, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "ax.set_xlabel(\"Beat Number\")\n",
    "ax.set_yticks(range(25))\n",
    "ax.set_yticklabels([f\"C{i}\" for i in range(12)] + [f\"MFCC{i}\" for i in range(13)], fontsize=7)\n",
    "ax.set_title(\"Beat-Synchronised Feature Matrix (Chroma + MFCC)\")\n",
    "fig.colorbar(img, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "torchaudio-header",
   "metadata": {},
   "source": [
    "## torchaudio: Transforms and Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "torchaudio-transforms",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = torch.tensor(y).unsqueeze(0).float()\n",
    "\n",
    "# Resample to 16 kHz (common for speech models)\n",
    "target_sr = 16000\n",
    "resampler = T.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "waveform_16k = resampler(waveform)\n",
    "\n",
    "# Mel Spectrogram\n",
    "mel_transform = T.MelSpectrogram(sample_rate=sr, n_fft=1024, hop_length=256, n_mels=80)\n",
    "to_db = T.AmplitudeToDB(stype='power', top_db=80)\n",
    "mel_db_torch = to_db(mel_transform(waveform))\n",
    "\n",
    "# Power Spectrogram\n",
    "spec_db_torch = to_db(T.Spectrogram(n_fft=1024, hop_length=256, power=2.0)(waveform))\n",
    "\n",
    "print(f\"Waveform        : {waveform.shape}\")\n",
    "print(f\"Resampled 16kHz : {waveform_16k.shape}\")\n",
    "print(f\"Mel Spectrogram : {mel_db_torch.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 7))\n",
    "axes[0].imshow(spec_db_torch[0].numpy(), aspect='auto', origin='lower',\n",
    "               cmap='magma', extent=[0, len(y)/sr, 0, sr/2])\n",
    "axes[0].set_xlabel(\"Time (s)\")\n",
    "axes[0].set_ylabel(\"Frequency (Hz)\")\n",
    "axes[0].set_title(\"torchaudio Spectrogram (dB)\")\n",
    "\n",
    "axes[1].imshow(mel_db_torch[0].numpy(), aspect='auto', origin='lower',\n",
    "               cmap='magma', extent=[0, len(y)/sr, 0, 80])\n",
    "axes[1].set_xlabel(\"Time (s)\")\n",
    "axes[1].set_ylabel(\"Mel Band\")\n",
    "axes[1].set_title(\"torchaudio Mel Spectrogram (80 mel bands, dB)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "1. **HPSS** separates harmonic and percussive layers — use them independently for cleaner features\n",
    "2. **Beat tracking** on the percussive layer gives stable tempo and beat positions\n",
    "3. **Mel Spectrogram + MFCC + Chroma** form a standard feature set for music analysis\n",
    "4. **Beat-synchronised features** reduce sequence length while preserving musical structure\n",
    "5. **torchaudio** provides GPU-accelerated transforms and easy integration with PyTorch pipelines\n",
    "\n",
    "**Resources:**\n",
    "- [librosa Tutorial](https://librosa.org/doc/latest/tutorial.html)\n",
    "- [torchaudio Transforms](https://pytorch.org/audio/stable/transforms.html)\n",
    "- [Music Information Retrieval (MIR)](https://musicinformationretrieval.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
