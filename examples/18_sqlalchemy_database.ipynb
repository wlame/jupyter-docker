{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# SQLAlchemy ORM and Columnar Data Formats\n",
    "\n",
    "Demonstrates SQLAlchemy 2.0 ORM with relationships and complex queries,\n",
    "plus high-performance columnar data I/O with PyArrow/Parquet and HDF5.\n",
    "\n",
    "**Libraries:**\n",
    "- [SQLAlchemy 2.0](https://docs.sqlalchemy.org/en/20/) — Python SQL toolkit and ORM with modern type hints\n",
    "- [PyArrow](https://arrow.apache.org/docs/python/) — Apache Arrow columnar in-memory format\n",
    "- [Parquet](https://arrow.apache.org/docs/python/parquet.html) — Columnar storage with predicate pushdown\n",
    "- [h5py](https://docs.h5py.org/) — HDF5 hierarchical scientific data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import (\n",
    "    create_engine, select, func, case, desc,\n",
    "    Integer, String, Float, DateTime, Boolean, ForeignKey,\n",
    ")\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship, Session\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import h5py\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "print(\"All imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orm-header",
   "metadata": {},
   "source": [
    "## SQLAlchemy 2.0 ORM: Define Models\n",
    "\n",
    "SQLAlchemy 2.0 uses `Mapped` type annotations — models are type-safe and IDE-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orm-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class Department(Base):\n",
    "    __tablename__ = 'department'\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    name: Mapped[str] = mapped_column(String(100), unique=True)\n",
    "    location: Mapped[Optional[str]] = mapped_column(String(100))\n",
    "    employees: Mapped[list['Employee']] = relationship(back_populates='department', cascade='all, delete-orphan')\n",
    "\n",
    "class Employee(Base):\n",
    "    __tablename__ = 'employee'\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    first_name: Mapped[str] = mapped_column(String(60))\n",
    "    last_name: Mapped[str] = mapped_column(String(60))\n",
    "    email: Mapped[str] = mapped_column(String(120), unique=True)\n",
    "    salary: Mapped[float] = mapped_column(Float)\n",
    "    hire_date: Mapped[datetime.date] = mapped_column(DateTime)\n",
    "    is_active: Mapped[bool] = mapped_column(Boolean, default=True)\n",
    "    department_id: Mapped[int] = mapped_column(ForeignKey('department.id'))\n",
    "    department: Mapped['Department'] = relationship(back_populates='employees')\n",
    "    projects: Mapped[list['ProjectAssignment']] = relationship(back_populates='employee', cascade='all, delete-orphan')\n",
    "\n",
    "class Project(Base):\n",
    "    __tablename__ = 'project'\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    name: Mapped[str] = mapped_column(String(120))\n",
    "    budget: Mapped[float] = mapped_column(Float)\n",
    "    status: Mapped[str] = mapped_column(String(20), default='active')\n",
    "    assignments: Mapped[list['ProjectAssignment']] = relationship(back_populates='project', cascade='all, delete-orphan')\n",
    "\n",
    "class ProjectAssignment(Base):\n",
    "    __tablename__ = 'project_assignment'\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    employee_id: Mapped[int] = mapped_column(ForeignKey('employee.id'))\n",
    "    project_id: Mapped[int] = mapped_column(ForeignKey('project.id'))\n",
    "    role: Mapped[str] = mapped_column(String(60))\n",
    "    hours_allocated: Mapped[float] = mapped_column(Float, default=0.0)\n",
    "    employee: Mapped['Employee'] = relationship(back_populates='projects')\n",
    "    project: Mapped['Project'] = relationship(back_populates='assignments')\n",
    "\n",
    "print(\"Models: Department, Employee, Project, ProjectAssignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orm-populate",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///:memory:', echo=False)\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "dept_data = [('Engineering','San Francisco'),('Marketing','New York'),\n",
    "             ('Finance','Chicago'),('Operations','Austin'),('Research','Boston')]\n",
    "first_names = ['Alice','Bob','Carol','David','Eve','Frank','Grace','Henry',\n",
    "               'Iris','Jack','Kate','Leo','Mia','Noah','Olivia','Paul','Quinn','Rachel']\n",
    "last_names = ['Smith','Jones','Brown','Williams','Taylor','Davies','Evans','Wilson']\n",
    "project_names = ['Project Apollo','Market Expansion','Cost Reduction','Digital Transformation',\n",
    "                 'Customer Portal','Data Platform','Mobile App','AI Initiative']\n",
    "\n",
    "with Session(engine) as session:\n",
    "    departments = [Department(name=n, location=l) for n, l in dept_data]\n",
    "    session.add_all(departments)\n",
    "    session.flush()\n",
    "\n",
    "    employees = []\n",
    "    base_date = datetime.date(2018, 1, 1)\n",
    "    salary_bases = {'Engineering': 110_000, 'Marketing': 85_000, 'Finance': 95_000,\n",
    "                    'Operations': 80_000, 'Research': 105_000}\n",
    "    for i in range(50):\n",
    "        dept = rng.choice(departments)\n",
    "        base = salary_bases[dept.name]\n",
    "        emp = Employee(\n",
    "            first_name=rng.choice(first_names), last_name=rng.choice(last_names),\n",
    "            email=f\"emp{i:03d}@co.com\",\n",
    "            salary=float(rng.normal(base, base * 0.12)),\n",
    "            hire_date=datetime.datetime.combine(\n",
    "                base_date + datetime.timedelta(days=int(rng.integers(0, 365*6))), datetime.time.min\n",
    "            ),\n",
    "            is_active=bool(rng.random() > 0.1),\n",
    "            department_id=dept.id,\n",
    "        )\n",
    "        employees.append(emp)\n",
    "    session.add_all(employees)\n",
    "    session.flush()\n",
    "\n",
    "    projects = [Project(name=n, budget=float(rng.uniform(50_000, 500_000)),\n",
    "                        status=rng.choice(['active','completed','active','active']))\n",
    "                for n in project_names]\n",
    "    session.add_all(projects)\n",
    "    session.flush()\n",
    "\n",
    "    for project in projects:\n",
    "        n_assigned = int(rng.integers(3, 9))\n",
    "        assigned = rng.choice(employees, size=min(n_assigned, len(employees)), replace=False)\n",
    "        for emp in assigned:\n",
    "            session.add(ProjectAssignment(\n",
    "                employee_id=emp.id, project_id=project.id,\n",
    "                role=rng.choice(['Lead','Developer','Analyst','Designer','Manager']),\n",
    "                hours_allocated=float(rng.uniform(40, 400)),\n",
    "            ))\n",
    "    session.commit()\n",
    "\n",
    "print(f\"Inserted: 5 departments, 50 employees, 8 projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "queries-header",
   "metadata": {},
   "source": [
    "## Complex ORM Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-dept",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Department headcount and salary stats\n",
    "with Session(engine) as session:\n",
    "    stmt = (\n",
    "        select(\n",
    "            Department.name, Department.location,\n",
    "            func.count(Employee.id).label('headcount'),\n",
    "            func.round(func.avg(Employee.salary), 0).label('avg_salary'),\n",
    "            func.round(func.max(Employee.salary), 0).label('max_salary'),\n",
    "        )\n",
    "        .join(Employee)\n",
    "        .where(Employee.is_active == True)\n",
    "        .group_by(Department.id)\n",
    "        .order_by(desc('avg_salary'))\n",
    "    )\n",
    "    rows = session.execute(stmt).all()\n",
    "\n",
    "dept_df = pd.DataFrame(rows, columns=['Department', 'Location', 'Headcount', 'Avg Salary', 'Max Salary'])\n",
    "dept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-rank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 2 earners per department using window functions\n",
    "with Session(engine) as session:\n",
    "    subq = (\n",
    "        select(\n",
    "            Employee.id, Employee.first_name, Employee.last_name, Employee.salary,\n",
    "            Department.name.label('dept_name'),\n",
    "            func.rank().over(partition_by=Department.id, order_by=desc(Employee.salary)).label('salary_rank'),\n",
    "        )\n",
    "        .join(Department)\n",
    "        .where(Employee.is_active == True)\n",
    "        .subquery()\n",
    "    )\n",
    "    top2 = session.execute(select(subq).where(subq.c.salary_rank <= 2).order_by(subq.c.dept_name)).all()\n",
    "\n",
    "top2_df = pd.DataFrame(top2, columns=['id','first','last','salary','dept','rank'])\n",
    "top2_df['name'] = top2_df['first'] + ' ' + top2_df['last']\n",
    "top2_df[['dept','rank','name','salary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-salary-bands",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary bands using CASE expression\n",
    "with Session(engine) as session:\n",
    "    salary_band = case(\n",
    "        (Employee.salary >= 120_000, 'Senior (≥$120k)'),\n",
    "        (Employee.salary >= 90_000, 'Mid ($90k–$120k)'),\n",
    "        else_='Junior (<$90k)',\n",
    "    )\n",
    "    stmt = (\n",
    "        select(salary_band.label('band'),\n",
    "               func.count(Employee.id).label('count'),\n",
    "               func.round(func.avg(Employee.salary), 0).label('avg'))\n",
    "        .group_by(salary_band)\n",
    "        .order_by(desc('avg'))\n",
    "    )\n",
    "    rows = session.execute(stmt).all()\n",
    "\n",
    "pd.DataFrame(rows, columns=['Band', 'Count', 'Avg Salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arrow-header",
   "metadata": {},
   "source": [
    "## PyArrow: Schema-First Columnar Data\n",
    "\n",
    "PyArrow tables are typed, immutable column stores. The `compute` module provides\n",
    "vectorized operations that work directly on Arrow memory — no pandas needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arrow-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 200_000\n",
    "currencies = ['USD', 'EUR', 'GBP', 'JPY']\n",
    "categories = ['Electronics', 'Groceries', 'Travel', 'Dining', 'Healthcare', 'Entertainment']\n",
    "\n",
    "amounts = rng.lognormal(mean=3.5, sigma=1.2, size=n_rows)\n",
    "is_fraud = rng.random(n_rows) < 0.02\n",
    "amounts[is_fraud] *= rng.uniform(3, 20, is_fraud.sum())\n",
    "\n",
    "start_ms = pd.Timestamp('2024-01-01').value // 10**6\n",
    "end_ms = pd.Timestamp('2024-12-31').value // 10**6\n",
    "\n",
    "table = pa.table({\n",
    "    'transaction_id': pa.array(np.arange(n_rows, dtype=np.int64)),\n",
    "    'customer_id': pa.array(rng.integers(1, 10_001, n_rows, dtype=np.int32)),\n",
    "    'amount': pa.array(amounts),\n",
    "    'currency': pa.array(rng.choice(currencies, n_rows)).dictionary_encode(),\n",
    "    'category': pa.array(rng.choice(categories, n_rows)).dictionary_encode(),\n",
    "    'timestamp': pa.array(rng.integers(start_ms, end_ms, n_rows).astype('int64'), type=pa.timestamp('ms')),\n",
    "    'is_fraud': pa.array(is_fraud),\n",
    "})\n",
    "\n",
    "print(f\"Table: {table.num_rows:,} rows × {table.num_columns} columns\")\n",
    "print(f\"\\nArrow compute (no pandas):\")\n",
    "print(f\"  Total amount : ${pc.sum(table['amount']).as_py():,.2f}\")\n",
    "print(f\"  Fraud rate   : {pc.sum(table['is_fraud']).as_py() / n_rows:.2%}\")\n",
    "print(f\"  Max amount   : ${pc.max(table['amount']).as_py():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parquet-header",
   "metadata": {},
   "source": [
    "## Parquet: Partitioned Write and Predicate Pushdown\n",
    "\n",
    "Predicate pushdown lets Parquet skip row groups that can't contain matching rows,\n",
    "dramatically reducing I/O for filtered reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    parquet_path = os.path.join(tmpdir, 'transactions.parquet')\n",
    "\n",
    "    # Write\n",
    "    t0 = time.perf_counter()\n",
    "    pq.write_table(table, parquet_path, compression='snappy', row_group_size=50_000, write_statistics=True)\n",
    "    write_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    size_mb = os.path.getsize(parquet_path) / 1e6\n",
    "    meta = pq.read_metadata(parquet_path)\n",
    "\n",
    "    print(f\"Write: {write_ms:.0f} ms  |  Size: {size_mb:.2f} MB  |  Row groups: {meta.num_row_groups}\")\n",
    "\n",
    "    # Predicate pushdown: read only fraud\n",
    "    t0 = time.perf_counter()\n",
    "    fraud_table = pq.read_table(parquet_path,\n",
    "                                 columns=['transaction_id', 'amount', 'category'],\n",
    "                                 filters=[('is_fraud', '=', True)])\n",
    "    read_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    print(f\"\\nPredicate pushdown (is_fraud=True):\")\n",
    "    print(f\"  Rows returned: {fraud_table.num_rows:,} ({fraud_table.num_rows/n_rows:.2%})\")\n",
    "    print(f\"  Read time    : {read_ms:.0f} ms  (skipped non-matching row groups)\")\n",
    "\n",
    "    # Performance comparison: CSV vs Parquet vs Feather\n",
    "    sample = table.slice(0, 100_000).to_pandas()\n",
    "    fmt_results = []\n",
    "    for fmt, write_fn, read_fn, ext in [\n",
    "        ('CSV',     lambda p: sample.to_csv(p, index=False),  pd.read_csv,     'csv'),\n",
    "        ('Parquet', lambda p: sample.to_parquet(p),           pd.read_parquet, 'parquet'),\n",
    "        ('Feather', lambda p: sample.to_feather(p),           pd.read_feather, 'feather'),\n",
    "    ]:\n",
    "        p = os.path.join(tmpdir, f'bench.{ext}')\n",
    "        t0 = time.perf_counter(); write_fn(p); w = (time.perf_counter()-t0)*1000\n",
    "        t0 = time.perf_counter(); read_fn(p); r = (time.perf_counter()-t0)*1000\n",
    "        fmt_results.append({'Format': fmt, 'Write (ms)': w, 'Read (ms)': r, 'Size (KB)': os.path.getsize(p)/1024})\n",
    "\n",
    "pd.DataFrame(fmt_results).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdf5-header",
   "metadata": {},
   "source": [
    "## HDF5: Hierarchical Numerical Storage\n",
    "\n",
    "HDF5 stores large numerical arrays in a file-system-like hierarchy with metadata attributes.\n",
    "It supports chunked storage and gzip compression for efficient access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "n_paths, n_steps = 500, 252\n",
    "mu, sigma = 0.08, 0.20\n",
    "dt = 1 / n_steps\n",
    "W = rng.standard_normal((n_paths, n_steps))\n",
    "paths = 100.0 * np.exp(np.cumsum((mu - 0.5*sigma**2)*dt + sigma*np.sqrt(dt)*W, axis=1))\n",
    "final_prices = paths[:, -1]\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as f:\n",
    "    hdf5_path = f.name\n",
    "\n",
    "with h5py.File(hdf5_path, 'w') as f:\n",
    "    f.attrs['description'] = 'Monte Carlo GBM simulation'\n",
    "\n",
    "    sim = f.create_group('simulation')\n",
    "    ds = sim.create_dataset('paths', data=paths, compression='gzip', compression_opts=4, chunks=True)\n",
    "    ds.attrs['shape'] = f'{n_paths} paths × {n_steps} steps'\n",
    "    sim.create_dataset('time_axis', data=np.linspace(0, 1, n_steps))\n",
    "\n",
    "    stats_grp = f.create_group('statistics')\n",
    "    stats_grp.create_dataset('final_prices', data=final_prices)\n",
    "    stats_grp.attrs['mean_final'] = float(final_prices.mean())\n",
    "    stats_grp.attrs['std_final'] = float(final_prices.std())\n",
    "    stats_grp.attrs['var_95'] = float(np.percentile(final_prices, 5))\n",
    "    stats_grp.attrs['var_99'] = float(np.percentile(final_prices, 1))\n",
    "\n",
    "size_mb = os.path.getsize(hdf5_path) / 1e6\n",
    "\n",
    "with h5py.File(hdf5_path, 'r') as f:\n",
    "    s = f['statistics']\n",
    "    print(f\"Monte Carlo GBM: μ={mu}, σ={sigma}, S₀=100\")\n",
    "    print(f\"File size       : {size_mb:.2f} MB\")\n",
    "    print(f\"Mean final price: ${s.attrs['mean_final']:.2f}\")\n",
    "    print(f\"Std final price : ${s.attrs['std_final']:.2f}\")\n",
    "    print(f\"VaR 95%         : ${s.attrs['var_95']:.2f}\")\n",
    "    print(f\"VaR 99%         : ${s.attrs['var_99']:.2f}\")\n",
    "\n",
    "    # Demonstrate partial read (only 10 of 500 paths)\n",
    "    partial = f['simulation/paths'][:10, :]\n",
    "    print(f\"\\nSlice read shape: {partial.shape}  ({partial.nbytes/1024:.1f} KB)\")\n",
    "\n",
    "os.unlink(hdf5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdf5-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t_sim = np.linspace(0, 1, n_steps)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Sample paths\n",
    "for path in paths[:50]:\n",
    "    axes[0].plot(t_sim, path, linewidth=0.4, alpha=0.4, color='steelblue')\n",
    "axes[0].plot(t_sim, paths.mean(axis=0), linewidth=2.5, color='red', label='Mean path')\n",
    "axes[0].axhline(y=100, color='black', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "axes[0].set_xlabel(\"Time (years)\")\n",
    "axes[0].set_ylabel(\"Price\")\n",
    "axes[0].set_title(f\"Monte Carlo GBM — {n_paths} paths\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Final price distribution\n",
    "axes[1].hist(final_prices, bins=40, color='steelblue', alpha=0.75, density=True, edgecolor='white')\n",
    "axes[1].axvline(x=np.percentile(final_prices, 5), color='red', linestyle='--', linewidth=2, label='VaR 95%')\n",
    "axes[1].axvline(x=np.percentile(final_prices, 1), color='darkred', linestyle='--', linewidth=2, label='VaR 99%')\n",
    "axes[1].axvline(x=final_prices.mean(), color='green', linestyle='-', linewidth=2, label='Mean')\n",
    "axes[1].set_xlabel(\"Final Price\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].set_title(\"Distribution of Final Prices\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "1. **SQLAlchemy 2.0** — `Mapped` type annotations make models IDE-friendly; relationships handle JOIN complexity; window functions in SQL work via SQLAlchemy's `over()`\n",
    "2. **PyArrow** — Zero-copy columnar in-memory format; `pc.compute` functions are vectorized and faster than pandas for simple aggregations\n",
    "3. **Parquet** — Columnar storage with predicate pushdown skips irrelevant row groups; Snappy compression reduces size by 60–80% vs CSV\n",
    "4. **HDF5** — Chunked, compressed hierarchical storage for large numerical arrays; supports partial reads without loading full dataset\n",
    "\n",
    "**Format selection guide:**\n",
    "| Format | Best for |\n",
    "|--------|----------|\n",
    "| CSV | Human-readable exchange, small datasets |\n",
    "| Parquet | Analytics workloads, columnar queries, data lakes |\n",
    "| Feather | Fast in-process pandas exchange |\n",
    "| HDF5 | Hierarchical scientific data, large numerical arrays |\n",
    "\n",
    "**Resources:**\n",
    "- [SQLAlchemy 2.0 ORM Guide](https://docs.sqlalchemy.org/en/20/orm/quickstart.html)\n",
    "- [PyArrow Cookbook](https://arrow.apache.org/cookbook/py/)\n",
    "- [h5py Quick Start](https://docs.h5py.org/en/stable/quick.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
