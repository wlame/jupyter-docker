{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# Data I/O and Serialization\n",
        "\n",
        "This notebook demonstrates data reading, writing, and serialization with various formats.\n",
        "\n",
        "**Libraries:**\n",
        "- [orjson](https://github.com/ijl/orjson) / [ujson](https://github.com/ultrajson/ultrajson) / [simplejson](https://simplejson.readthedocs.io/) - Fast JSON libraries\n",
        "- [lxml](https://lxml.de/) / [xmltodict](https://github.com/martinblech/xmltodict) - XML processing\n",
        "- [PyYAML](https://pyyaml.org/) - YAML processing\n",
        "- [openpyxl](https://openpyxl.readthedocs.io/) - Excel files\n",
        "- [PyArrow](https://arrow.apache.org/docs/python/) / [fastparquet](https://fastparquet.readthedocs.io/) - Parquet files\n",
        "- [h5py](https://www.h5py.org/) / [PyTables](https://www.pytables.org/) - HDF5 files\n",
        "- [requests](https://requests.readthedocs.io/) / [httpx](https://www.python-httpx.org/) - HTTP clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## JSON Libraries Comparison\n",
        "\n",
        "Python has several JSON libraries with different performance characteristics:\n",
        "- **orjson**: Fastest, returns bytes, supports numpy arrays\n",
        "- **ujson**: Very fast, drop-in replacement for json\n",
        "- **simplejson**: Feature-rich, compatible with json module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import orjson\n",
        "import ujson\n",
        "import simplejson\n",
        "\n",
        "# Sample data\n",
        "sample_data = {\n",
        "    \"users\": [\n",
        "        {\"id\": 1, \"name\": \"Alice\", \"scores\": [95, 87, 92]},\n",
        "        {\"id\": 2, \"name\": \"Bob\", \"scores\": [78, 85, 90]},\n",
        "    ],\n",
        "    \"metadata\": {\"version\": \"1.0\", \"generated\": \"2024-01-01\"},\n",
        "    \"count\": 2,\n",
        "}\n",
        "sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library json\n",
        "json_str = json.dumps(sample_data, indent=2)\n",
        "print(\"Standard json library:\")\n",
        "print(json_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# orjson (fastest, returns bytes)\n",
        "orjson_bytes = orjson.dumps(sample_data)\n",
        "print(f\"orjson output (bytes): {orjson_bytes}\")\n",
        "\n",
        "# Pretty print with orjson\n",
        "orjson_pretty = orjson.dumps(sample_data, option=orjson.OPT_INDENT_2).decode(\"utf-8\")\n",
        "print(f\"\\norjson pretty printed:\\n{orjson_pretty}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ujson and simplejson\n",
        "ujson_str = ujson.dumps(sample_data, indent=2)\n",
        "simplejson_str = simplejson.dumps(sample_data, indent=2)\n",
        "\n",
        "print(\"ujson output:\")\n",
        "print(ujson_str[:100] + \"...\")\n",
        "\n",
        "# Parsing\n",
        "parsed = orjson.loads(orjson_bytes)\n",
        "print(f\"\\nParsed data keys: {list(parsed.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {},
      "source": [
        "## XML Libraries\n",
        "\n",
        "XML processing options:\n",
        "- **lxml**: Fast, feature-rich XML/HTML processing\n",
        "- **xmltodict**: Converts XML to Python dictionaries\n",
        "- **ElementTree**: Standard library XML parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from lxml import etree\n",
        "import xmltodict\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Sample XML\n",
        "xml_string = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<catalog>\n",
        "    <book id=\"1\">\n",
        "        <title>Python Programming</title>\n",
        "        <author>John Doe</author>\n",
        "        <price>29.99</price>\n",
        "    </book>\n",
        "    <book id=\"2\">\n",
        "        <title>Data Science Handbook</title>\n",
        "        <author>Jane Smith</author>\n",
        "        <price>39.99</price>\n",
        "    </book>\n",
        "</catalog>\n",
        "\"\"\"\n",
        "print(xml_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# lxml parsing\n",
        "print(\"lxml parsing:\")\n",
        "root = etree.fromstring(xml_string.encode())\n",
        "for book in root.findall(\"book\"):\n",
        "    title = book.find(\"title\").text\n",
        "    author = book.find(\"author\").text\n",
        "    price = book.find(\"price\").text\n",
        "    print(f\"  {title} by {author} - ${price}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# xmltodict - convert XML to dictionary\n",
        "print(\"xmltodict conversion:\")\n",
        "xml_dict = xmltodict.parse(xml_string)\n",
        "print(f\"Type: {type(xml_dict)}\")\n",
        "print(f\"Books: {len(xml_dict['catalog']['book'])}\")\n",
        "\n",
        "# Access like a dictionary\n",
        "for book in xml_dict['catalog']['book']:\n",
        "    print(f\"  Book ID {book['@id']}: {book['title']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library ElementTree\n",
        "print(\"ElementTree parsing:\")\n",
        "et_root = ET.fromstring(xml_string)\n",
        "for book in et_root.findall(\"book\"):\n",
        "    book_id = book.get(\"id\")\n",
        "    author = book.find(\"author\").text\n",
        "    print(f\"  Book {book_id}: by {author}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "## YAML Processing\n",
        "\n",
        "YAML is commonly used for configuration files due to its human-readable format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "yaml_data = \"\"\"\n",
        "database:\n",
        "  host: localhost\n",
        "  port: 5432\n",
        "  credentials:\n",
        "    username: admin\n",
        "    password: secret\n",
        "\n",
        "servers:\n",
        "  - name: web1\n",
        "    ip: 192.168.1.1\n",
        "  - name: web2\n",
        "    ip: 192.168.1.2\n",
        "\n",
        "features:\n",
        "  enabled: true\n",
        "  max_connections: 100\n",
        "\"\"\"\n",
        "\n",
        "# Parse YAML\n",
        "config = yaml.safe_load(yaml_data)\n",
        "print(\"Parsed YAML config:\")\n",
        "print(f\"  Database host: {config['database']['host']}\")\n",
        "print(f\"  Number of servers: {len(config['servers'])}\")\n",
        "print(f\"  Features enabled: {config['features']['enabled']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dump back to YAML\n",
        "output_yaml = yaml.dump(config, default_flow_style=False)\n",
        "print(\"Dumped YAML:\")\n",
        "print(output_yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {},
      "source": [
        "## Excel File Handling\n",
        "\n",
        "Read and write Excel files using openpyxl (for .xlsx) and pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {},
      "outputs": [],
      "source": [
        "import openpyxl\n",
        "\n",
        "# Create sample DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
        "    \"Age\": [25, 30, 35],\n",
        "    \"Salary\": [50000, 60000, 70000],\n",
        "})\n",
        "\n",
        "# Write to Excel\n",
        "with tempfile.NamedTemporaryFile(suffix=\".xlsx\", delete=False) as f:\n",
        "    excel_path = f.name\n",
        "\n",
        "df.to_excel(excel_path, sheet_name=\"Employees\", index=False)\n",
        "print(f\"Written DataFrame to Excel: {excel_path}\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read back with pandas\n",
        "df_read = pd.read_excel(excel_path, sheet_name=\"Employees\")\n",
        "print(f\"Read back DataFrame shape: {df_read.shape}\")\n",
        "df_read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using openpyxl directly\n",
        "wb = openpyxl.load_workbook(excel_path)\n",
        "ws = wb.active\n",
        "print(f\"Sheet name: {ws.title}\")\n",
        "print(f\"Max row: {ws.max_row}, Max col: {ws.max_column}\")\n",
        "\n",
        "# Read cell values\n",
        "print(\"\\nCell values:\")\n",
        "for row in ws.iter_rows(min_row=1, max_row=2, values_only=True):\n",
        "    print(f\"  {row}\")\n",
        "\n",
        "os.unlink(excel_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {},
      "source": [
        "## Parquet File Handling\n",
        "\n",
        "Parquet is a columnar storage format optimized for analytics workloads.\n",
        "\n",
        "**Advantages:**\n",
        "- Efficient compression\n",
        "- Column pruning (read only needed columns)\n",
        "- Predicate pushdown\n",
        "- Schema preservation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import fastparquet\n",
        "\n",
        "# Create larger DataFrame\n",
        "np.random.seed(42)\n",
        "df_large = pd.DataFrame({\n",
        "    \"id\": range(10000),\n",
        "    \"value\": np.random.randn(10000),\n",
        "    \"category\": np.random.choice([\"A\", \"B\", \"C\"], 10000),\n",
        "    \"date\": pd.date_range(\"2024-01-01\", periods=10000, freq=\"h\"),\n",
        "})\n",
        "\n",
        "print(f\"DataFrame shape: {df_large.shape}\")\n",
        "df_large.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write with PyArrow\n",
        "with tempfile.NamedTemporaryFile(suffix=\".parquet\", delete=False) as f:\n",
        "    parquet_path = f.name\n",
        "\n",
        "df_large.to_parquet(parquet_path, engine=\"pyarrow\", compression=\"snappy\")\n",
        "parquet_size = os.path.getsize(parquet_path)\n",
        "print(f\"Parquet file size: {parquet_size / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read with PyArrow\n",
        "df_parquet = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
        "print(f\"Full read shape: {df_parquet.shape}\")\n",
        "\n",
        "# Read specific columns only (column pruning)\n",
        "df_partial = pd.read_parquet(parquet_path, columns=[\"id\", \"value\"])\n",
        "print(f\"Partial read (2 columns): {df_partial.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parquet metadata and schema\n",
        "parquet_file = pq.read_table(parquet_path)\n",
        "print(\"Parquet schema:\")\n",
        "print(parquet_file.schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# FastParquet alternative\n",
        "df_fp = fastparquet.ParquetFile(parquet_path).to_pandas()\n",
        "print(f\"Fastparquet read shape: {df_fp.shape}\")\n",
        "\n",
        "os.unlink(parquet_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-25",
      "metadata": {},
      "source": [
        "## HDF5 File Handling\n",
        "\n",
        "HDF5 is designed for storing large amounts of scientific data with hierarchical structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-26",
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "import tables\n",
        "\n",
        "# Create HDF5 with h5py\n",
        "with tempfile.NamedTemporaryFile(suffix=\".h5\", delete=False) as f:\n",
        "    hdf5_path = f.name\n",
        "\n",
        "# Write data with h5py\n",
        "with h5py.File(hdf5_path, \"w\") as f:\n",
        "    # Create datasets\n",
        "    f.create_dataset(\"data\", data=np.random.randn(1000, 100))\n",
        "    f.create_dataset(\"labels\", data=np.random.randint(0, 10, 1000))\n",
        "    \n",
        "    # Create groups with attributes\n",
        "    grp = f.create_group(\"metadata\")\n",
        "    grp.attrs[\"version\"] = \"1.0\"\n",
        "    grp.attrs[\"description\"] = \"Sample HDF5 file\"\n",
        "\n",
        "print(\"Written HDF5 file with h5py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read with h5py\n",
        "with h5py.File(hdf5_path, \"r\") as f:\n",
        "    print(\"HDF5 structure:\")\n",
        "    print(f\"  Keys: {list(f.keys())}\")\n",
        "    print(f\"  Data shape: {f['data'].shape}\")\n",
        "    print(f\"  Labels shape: {f['labels'].shape}\")\n",
        "    print(f\"  Metadata version: {f['metadata'].attrs['version']}\")\n",
        "    \n",
        "    # Read a slice of data\n",
        "    data_slice = f['data'][:10, :5]\n",
        "    print(f\"  Data slice shape: {data_slice.shape}\")\n",
        "\n",
        "os.unlink(hdf5_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pandas HDFStore (uses PyTables)\n",
        "with tempfile.NamedTemporaryFile(suffix=\".h5\", delete=False) as f:\n",
        "    hdf_pandas_path = f.name\n",
        "\n",
        "# Write DataFrame to HDF5\n",
        "df_large.to_hdf(hdf_pandas_path, key=\"data\", mode=\"w\", complevel=5)\n",
        "print(f\"Written DataFrame to HDF5: {os.path.getsize(hdf_pandas_path) / 1024:.2f} KB\")\n",
        "\n",
        "# Read back\n",
        "df_hdf = pd.read_hdf(hdf_pandas_path, key=\"data\")\n",
        "print(f\"Read back shape: {df_hdf.shape}\")\n",
        "\n",
        "os.unlink(hdf_pandas_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-29",
      "metadata": {},
      "source": [
        "## HTTP Client Libraries\n",
        "\n",
        "Python HTTP clients for API requests:\n",
        "- **requests**: Simple, synchronous HTTP library\n",
        "- **httpx**: Modern, async-capable HTTP client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-30",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import httpx\n",
        "\n",
        "# Note: These examples show API usage patterns\n",
        "print(\"requests library examples:\")\n",
        "print(\"  GET:  response = requests.get('https://api.example.com/data')\")\n",
        "print(\"  POST: response = requests.post(url, json={'key': 'value'})\")\n",
        "print(\"  Headers: requests.get(url, headers={'Authorization': 'Bearer token'})\")\n",
        "\n",
        "print(\"\\nhttpx library examples (async capable):\")\n",
        "print(\"  Sync:  response = httpx.get('https://api.example.com/data')\")\n",
        "print(\"  Async: async with httpx.AsyncClient() as client:\")\n",
        "print(\"           response = await client.get(url)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-31",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Session management with requests\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"DataScience-Notebook/1.0\"})\n",
        "print(f\"Session headers: {dict(session.headers)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-32",
      "metadata": {},
      "source": [
        "## Pandas I/O Capabilities\n",
        "\n",
        "Compare file sizes and read times for different formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data\n",
        "df_sample = pd.DataFrame({\n",
        "    \"date\": pd.date_range(\"2024-01-01\", periods=100),\n",
        "    \"value\": np.random.randn(100),\n",
        "    \"category\": np.random.choice([\"X\", \"Y\", \"Z\"], 100),\n",
        "})\n",
        "\n",
        "with tempfile.TemporaryDirectory() as tmpdir:\n",
        "    # CSV\n",
        "    csv_path = os.path.join(tmpdir, \"data.csv\")\n",
        "    df_sample.to_csv(csv_path, index=False)\n",
        "    csv_size = os.path.getsize(csv_path)\n",
        "    \n",
        "    # JSON (Pandas)\n",
        "    json_path = os.path.join(tmpdir, \"data.json\")\n",
        "    df_sample.to_json(json_path, orient=\"records\", date_format=\"iso\")\n",
        "    json_size = os.path.getsize(json_path)\n",
        "    \n",
        "    # Feather (fast binary format)\n",
        "    feather_path = os.path.join(tmpdir, \"data.feather\")\n",
        "    df_sample.to_feather(feather_path)\n",
        "    feather_size = os.path.getsize(feather_path)\n",
        "    \n",
        "    print(\"File size comparison:\")\n",
        "    print(f\"  CSV:     {csv_size:,} bytes\")\n",
        "    print(f\"  JSON:    {json_size:,} bytes\")\n",
        "    print(f\"  Feather: {feather_size:,} bytes\")\n",
        "    \n",
        "    # Read back\n",
        "    df_csv = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
        "    df_json = pd.read_json(json_path)\n",
        "    df_feather = pd.read_feather(feather_path)\n",
        "    \n",
        "    print(f\"\\nAll formats read successfully with shape: {df_csv.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-34",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this notebook, we covered:\n",
        "\n",
        "1. **JSON Libraries**: orjson, ujson, simplejson for fast JSON serialization\n",
        "2. **XML Processing**: lxml, xmltodict, ElementTree\n",
        "3. **YAML**: PyYAML for configuration files\n",
        "4. **Excel**: openpyxl and pandas for spreadsheet I/O\n",
        "5. **Parquet**: PyArrow and fastparquet for columnar storage\n",
        "6. **HDF5**: h5py and pandas HDFStore for scientific data\n",
        "7. **HTTP Clients**: requests and httpx for API access\n",
        "\n",
        "**Format Selection Guide:**\n",
        "- **CSV**: Human-readable, universal compatibility\n",
        "- **JSON**: API data, configuration, web applications\n",
        "- **Parquet**: Analytics, big data, columnar queries\n",
        "- **HDF5**: Scientific computing, large arrays, hierarchical data\n",
        "- **Feather**: Fast DataFrame serialization between Python/R"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
