{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# NLP Text Analysis\n",
    "\n",
    "Demonstrates natural language processing with spaCy, NLTK, and sentence-transformers.\n",
    "\n",
    "**Libraries:**\n",
    "- [spaCy](https://spacy.io/) — Industrial-strength NLP: NER, dependency parsing, tokenization\n",
    "- [NLTK](https://www.nltk.org/) — Classic NLP toolkit: VADER sentiment, WordNet, stemming\n",
    "- [Sentence Transformers](https://www.sbert.net/) — Semantic embeddings for search and clustering\n",
    "\n",
    "> **Note:** spaCy model required — run once:  \n",
    "> `python -m spacy download en_core_web_sm`  \n",
    "> The sentence transformer model (~80 MB) downloads automatically on first use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "for resource in ('punkt', 'punkt_tab', 'stopwords', 'vader_lexicon', 'wordnet',\n",
    "                  'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng'):\n",
    "    nltk.download(resource, quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = [\n",
    "    \"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in Cupertino, California in 1976.\",\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "    \"NASA's Mars rover Perseverance successfully landed in Jezero Crater on February 18, 2021.\",\n",
    "    \"Albert Einstein was a German-born theoretical physicist who developed the theory of relativity.\",\n",
    "    \"The Amazon River flows through nine nations in South America and discharges into the Atlantic Ocean.\",\n",
    "]\n",
    "\n",
    "REVIEWS = [\n",
    "    \"This product is absolutely fantastic! I love everything about it. Highly recommended!\",\n",
    "    \"Terrible experience. The item broke after one day. Complete waste of money.\",\n",
    "    \"It's okay, nothing special. Does what it's supposed to do, I guess.\",\n",
    "    \"Outstanding quality and fast shipping. Will definitely buy again!\",\n",
    "    \"Not what I expected. The description was misleading and customer service was unhelpful.\",\n",
    "    \"Pretty good overall. A few minor issues but nothing dealbreaking.\",\n",
    "    \"Worst purchase ever. Avoid at all costs. Save your money.\",\n",
    "    \"Exceeded all expectations. Remarkable craftsmanship and attention to detail.\",\n",
    "]\n",
    "\n",
    "print(f\"Corpus: {len(TEXTS)} fact sentences, {len(REVIEWS)} product reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spacy-header",
   "metadata": {},
   "source": [
    "## spaCy: Named Entity Recognition and Dependency Parsing\n",
    "\n",
    "spaCy's NLP pipeline processes text into `Doc` objects with rich linguistic annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spacy-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print(\"Downloading en_core_web_sm...\")\n",
    "    subprocess.run([sys.executable, '-m', 'spacy', 'download', 'en_core_web_sm'],\n",
    "                   check=True, capture_output=True)\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(f\"Pipeline components: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spacy-ner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "all_entities = []\n",
    "for text in TEXTS:\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_, spacy.explain(ent.label_)) for ent in doc.ents]\n",
    "    all_entities.extend(entities)\n",
    "    print(f\"\\n{text[:75]}...\")\n",
    "    for ent_text, label, explanation in entities:\n",
    "        print(f\"  [{label:8s}] {ent_text!r:35s} — {explanation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spacy-dep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency parsing on sentence 1\n",
    "doc = nlp(TEXTS[0])\n",
    "print(f\"Sentence: {TEXTS[0]}\\n\")\n",
    "print(f\"{'Token':<20} {'Dep':<14} {'POS':<10} {'Head'}\")\n",
    "print(\"-\" * 60)\n",
    "for token in doc:\n",
    "    if not token.is_punct:\n",
    "        print(f\"{token.text:<20} {token.dep_:<14} {token.pos_:<10} {token.head.text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spacy-tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-level linguistic features\n",
    "print(f\"{'Token':<20} {'Lemma':<20} {'POS':<10} {'Is Stop'}\")\n",
    "print(\"-\" * 60)\n",
    "for token in doc:\n",
    "    if not token.is_space and not token.is_punct:\n",
    "        print(f\"{token.text:<20} {token.lemma_:<20} {token.pos_:<10} {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nltk-header",
   "metadata": {},
   "source": [
    "## NLTK: Tokenization, Preprocessing, and Sentiment Analysis\n",
    "\n",
    "NLTK provides classical NLP primitives and the VADER lexicon-based sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nltk-preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text = \"The quick brown foxes were jumping over the lazily sleeping dogs in the park.\"\n",
    "words = word_tokenize(text)\n",
    "filtered = [w for w in words if w.isalpha() and w.lower() not in stop_words]\n",
    "lemmatized = [lemmatizer.lemmatize(w.lower()) for w in filtered]\n",
    "stemmed = [stemmer.stem(w.lower()) for w in filtered]\n",
    "\n",
    "print(f\"Original  : {text}\")\n",
    "print(f\"Tokens    : {words}\")\n",
    "print(f\"Filtered  : {filtered}\")\n",
    "print(f\"Lemmatized: {lemmatized}\")\n",
    "print(f\"Stemmed   : {stemmed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nltk-vader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER sentiment analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiments = []\n",
    "\n",
    "print(f\"{'Review':<55} {'Compound':>9} {'Label':>10}\")\n",
    "print(\"-\" * 77)\n",
    "for review in REVIEWS:\n",
    "    scores = sia.polarity_scores(review)\n",
    "    compound = scores['compound']\n",
    "    label = 'positive' if compound >= 0.05 else ('negative' if compound <= -0.05 else 'neutral')\n",
    "    sentiments.append({'text': review, 'compound': compound, 'label': label, **scores})\n",
    "    short = review[:53] + '..' if len(review) > 55 else review\n",
    "    print(f\"{short:<55} {compound:>9.3f} {label:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nltk-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['positive', 'neutral', 'negative']\n",
    "counts = [sum(1 for s in sentiments if s['label'] == l) for l in labels]\n",
    "colors = ['#2ecc71', '#95a5a6', '#e74c3c']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].bar(labels, counts, color=colors, edgecolor='black', alpha=0.85)\n",
    "axes[0].set_title(\"Sentiment Distribution (VADER)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "compound_scores = [s['compound'] for s in sentiments]\n",
    "bar_colors = [colors[0] if c >= 0.05 else (colors[2] if c <= -0.05 else colors[1]) for c in compound_scores]\n",
    "axes[1].barh(range(len(compound_scores)), compound_scores, color=bar_colors, edgecolor='black', alpha=0.85)\n",
    "axes[1].axvline(x=0, color='black', linewidth=0.8, linestyle='--')\n",
    "axes[1].axvline(x=0.05, color='green', linewidth=0.8, linestyle=':')\n",
    "axes[1].axvline(x=-0.05, color='red', linewidth=0.8, linestyle=':')\n",
    "axes[1].set_yticks(range(len(REVIEWS)))\n",
    "axes[1].set_yticklabels([f\"Review {i+1}\" for i in range(len(REVIEWS))], fontsize=9)\n",
    "axes[1].set_xlabel(\"Compound Score\")\n",
    "axes[1].set_title(\"Per-Review Compound Scores\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wordnet-header",
   "metadata": {},
   "source": [
    "## NLTK: WordNet — Synonyms, Antonyms, and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wordnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['happy', 'fast', 'beautiful']:\n",
    "    synsets = wordnet.synsets(word)\n",
    "    print(f\"\\n'{word}' — {len(synsets)} synset(s):\")\n",
    "    synonyms, antonyms = set(), set()\n",
    "    for syn in synsets[:3]:\n",
    "        print(f\"  [{syn.name()}] {syn.definition()}\")\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "            for ant in lemma.antonyms():\n",
    "                antonyms.add(ant.name().replace('_', ' '))\n",
    "    print(f\"  Synonyms : {', '.join(sorted(synonyms)[:8])}\")\n",
    "    if antonyms:\n",
    "        print(f\"  Antonyms : {', '.join(sorted(antonyms))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sbert-header",
   "metadata": {},
   "source": [
    "## Sentence Transformers: Semantic Similarity and Search\n",
    "\n",
    "Unlike keyword matching, sentence embeddings capture *meaning*. Similar sentences cluster\n",
    "together in vector space regardless of exact wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sbert-encode",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "corpus_embeddings = model.encode(TEXTS)\n",
    "print(f\"Encoded {len(TEXTS)} sentences → shape {corpus_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sbert-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = [\n",
    "    \"Who founded Apple?\",\n",
    "    \"Where is the Eiffel Tower located?\",\n",
    "    \"What did Einstein develop?\",\n",
    "]\n",
    "\n",
    "query_embeddings = model.encode(QUERIES)\n",
    "for query, q_emb in zip(QUERIES, query_embeddings):\n",
    "    scores = cosine_similarity([q_emb], corpus_embeddings)[0]\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    print(f\"\\nQuery : {query!r}\")\n",
    "    print(f\"Match : [{scores[best_idx]:.3f}] {TEXTS[best_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sbert-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = cosine_similarity(corpus_embeddings)\n",
    "short_labels = [t[:35] + '...' for t in TEXTS]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "im = ax.imshow(sim_matrix, cmap='Blues', vmin=0, vmax=1)\n",
    "plt.colorbar(im, ax=ax, label='Cosine Similarity')\n",
    "ax.set_xticks(range(len(TEXTS)))\n",
    "ax.set_yticks(range(len(TEXTS)))\n",
    "ax.set_xticklabels(short_labels, rotation=30, ha='right', fontsize=8)\n",
    "ax.set_yticklabels(short_labels, fontsize=8)\n",
    "ax.set_title(\"Sentence Similarity Heatmap (all-MiniLM-L6-v2)\")\n",
    "for i in range(len(TEXTS)):\n",
    "    for j in range(len(TEXTS)):\n",
    "        ax.text(j, i, f\"{sim_matrix[i,j]:.2f}\", ha='center', va='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sbert-cluster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster reviews into sentiment groups using embeddings\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "review_embeddings = model.encode(REVIEWS)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(review_embeddings)\n",
    "\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label == cluster_id:\n",
    "            compound = sentiments[idx]['compound']\n",
    "            print(f\"  [{compound:+.3f}] {REVIEWS[idx][:70]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "1. **spaCy** — NER extracts named entities (persons, places, orgs, dates); dependency parsing reveals sentence grammar\n",
    "2. **NLTK** — VADER gives fast, lexicon-based sentiment without model training; WordNet provides a rich lexical database\n",
    "3. **Sentence Transformers** — Dense embeddings enable semantic search and meaning-aware clustering\n",
    "\n",
    "**Resources:**\n",
    "- [spaCy Usage Guide](https://spacy.io/usage)\n",
    "- [NLTK Book (free)](https://www.nltk.org/book/)\n",
    "- [SBERT Documentation](https://www.sbert.net/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
